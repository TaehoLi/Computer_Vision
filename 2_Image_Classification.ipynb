{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. mnist tf perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy :  0.8011\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "input_size = 784\n",
    "no_classes = 10\n",
    "batch_size = 100\n",
    "total_batches = 200\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([input_size, no_classes]))\n",
    "bias = tf.Variable(tf.random_normal([no_classes]))\n",
    "\n",
    "logits = tf.matmul(x_input, weights) + bias\n",
    "\n",
    "softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=y_input, logits=logits)\n",
    "loss_operation = tf.reduce_mean(softmax_cross_entropy)\n",
    "optimiser = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=0.5).minimize(loss_operation)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for batch_no in range(total_batches):\n",
    "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
    "    train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
    "    _, loss_value = session.run([optimiser, loss_operation], feed_dict={\n",
    "        x_input: train_images,\n",
    "        y_input: train_labels\n",
    "    })\n",
    "    #print(loss_value)\n",
    "\n",
    "predictions = tf.argmax(logits, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions,\n",
    "                                            tf.float32))\n",
    "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
    "accuracy_value = session.run(accuracy_operation, feed_dict={\n",
    "    x_input: test_images,\n",
    "    y_input: test_labels\n",
    "})\n",
    "print('Accuracy : ', accuracy_value)\n",
    "session.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. mnist tf cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-99f13aba56c4>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-99f13aba56c4>:82: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "TensorBoard 1.12.2 at http://ubuntu16:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "input_size = 784\n",
    "no_classes = 10\n",
    "batch_size = 100\n",
    "total_batches = 200\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])\n",
    "\n",
    "\n",
    "def add_variable_summary(tf_variable, summary_name):\n",
    "    with tf.name_scope(summary_name + '_summary'):\n",
    "        mean = tf.reduce_mean(tf_variable)\n",
    "        tf.summary.scalar('Mean', mean)\n",
    "        with tf.name_scope('standard_deviation'):\n",
    "            standard_deviation = tf.sqrt(tf.reduce_mean(\n",
    "                tf.square(tf_variable - mean)))\n",
    "        tf.summary.scalar('StandardDeviation', standard_deviation)\n",
    "        tf.summary.scalar('Maximum', tf.reduce_max(tf_variable))\n",
    "        tf.summary.scalar('Minimum', tf.reduce_min(tf_variable))\n",
    "        tf.summary.histogram('Histogram', tf_variable)\n",
    "\n",
    "\n",
    "x_input_reshape = tf.reshape(x_input, [-1, 28, 28, 1],\n",
    "                             name='input_reshape')\n",
    "\n",
    "\n",
    "def convolution_layer(input_layer, filters, kernel_size=[3, 3],\n",
    "                      activation=tf.nn.relu):\n",
    "    layer = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        activation=activation\n",
    "    )\n",
    "    add_variable_summary(layer, 'convolution')\n",
    "    return layer\n",
    "\n",
    "\n",
    "def pooling_layer(input_layer, pool_size=[2, 2], strides=2):\n",
    "    layer = tf.layers.max_pooling2d(\n",
    "        inputs=input_layer,\n",
    "        pool_size=pool_size,\n",
    "        strides=strides\n",
    "    )\n",
    "    add_variable_summary(layer, 'pooling')\n",
    "    return layer\n",
    "\n",
    "\n",
    "def dense_layer(input_layer, units, activation=tf.nn.relu):\n",
    "    layer = tf.layers.dense(\n",
    "        inputs=input_layer,\n",
    "        units=units,\n",
    "        activation=activation\n",
    "    )\n",
    "    add_variable_summary(layer, 'dense')\n",
    "    return layer\n",
    "\n",
    "\n",
    "convolution_layer_1 = convolution_layer(x_input_reshape, 64)\n",
    "pooling_layer_1 = pooling_layer(convolution_layer_1)\n",
    "convolution_layer_2 = convolution_layer(pooling_layer_1, 128)\n",
    "pooling_layer_2 = pooling_layer(convolution_layer_2)\n",
    "flattened_pool = tf.reshape(pooling_layer_2, [-1, 5 * 5 * 128],\n",
    "                            name='flattened_pool')\n",
    "dense_layer_bottleneck = dense_layer(flattened_pool, 1024)\n",
    "\n",
    "dropout_bool = tf.placeholder(tf.bool)\n",
    "dropout_layer = tf.layers.dropout(\n",
    "        inputs=dense_layer_bottleneck,\n",
    "        rate=0.4,\n",
    "        training=dropout_bool\n",
    "    )\n",
    "logits = dense_layer(dropout_layer, no_classes)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y_input, logits=logits)\n",
    "    loss_operation = tf.reduce_mean(softmax_cross_entropy, name='loss')\n",
    "    tf.summary.scalar('loss', loss_operation)\n",
    "\n",
    "with tf.name_scope('optimiser'):\n",
    "    optimiser = tf.train.AdamOptimizer().minimize(loss_operation)\n",
    "\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy_operation = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy_operation)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary_operation = tf.summary.merge_all()\n",
    "train_summary_writer = tf.summary.FileWriter('/tmp/train', session.graph) # tensorboard\n",
    "test_summary_writer = tf.summary.FileWriter('/tmp/test') # tensorboard\n",
    "\n",
    "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
    "\n",
    "for batch_no in range(total_batches):\n",
    "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
    "    train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
    "    _, merged_summary = session.run([optimiser, merged_summary_operation],\n",
    "                                    feed_dict={\n",
    "        x_input: train_images,\n",
    "        y_input: train_labels,\n",
    "        dropout_bool: True\n",
    "    })\n",
    "    train_summary_writer.add_summary(merged_summary, batch_no)\n",
    "    if batch_no % 10 == 0:\n",
    "        merged_summary, _ = session.run([merged_summary_operation,\n",
    "                                         accuracy_operation], feed_dict={\n",
    "            x_input: test_images,\n",
    "            y_input: test_labels,\n",
    "            dropout_bool: False\n",
    "        })\n",
    "        test_summary_writer.add_summary(merged_summary, batch_no)\n",
    "\n",
    "session.close()\n",
    "\n",
    "!tensorboard --logdir=/tmp/train\n",
    "#!tensorboard --logdir=/tmp/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. mnist keras cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Train data loss: 0.001994904852285125\n",
      "Train data accuracy: 0.9993166666666666\n",
      "Test data loss: 0.038808715384182735\n",
      "Test data accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128\n",
    "no_classes = 10\n",
    "epochs = 10\n",
    "image_height, image_width = 28, 28\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], image_height, image_width, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_height, image_width, 1)\n",
    "input_shape = (image_height, image_width, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, no_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, no_classes)\n",
    "\n",
    "\n",
    "def simple_cnn(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Dense(units=no_classes, activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "simple_cnn_model = simple_cnn(input_shape)\n",
    "\n",
    "simple_cnn_model.fit(x_train, y_train, batch_size, epochs, (x_test, y_test))\n",
    "train_loss, train_accuracy = simple_cnn_model.evaluate(\n",
    "    x_train, y_train, verbose=0)\n",
    "print('Train data loss:', train_loss)\n",
    "print('Train data accuracy:', train_accuracy)\n",
    "\n",
    "test_loss, test_accuracy = simple_cnn_model.evaluate(\n",
    "    x_test, y_test, verbose=0)\n",
    "print('Test data loss:', test_loss)\n",
    "print('Test data accuracy:', test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. cat vs dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare data\n",
    "download data from here: https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is prepared\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "work_dir = 'dataset'\n",
    "\n",
    "\n",
    "image_names = sorted(os.listdir(os.path.join(work_dir, 'train')))\n",
    "\n",
    "def copy_files(prefix_str, range_start, range_end, target_dir):\n",
    "    image_paths = [os.path.join(work_dir, 'train', prefix_str + '.' + str(i) + '.jpg')\n",
    "                   for i in range(range_start, range_end)]\n",
    "    dest_dir = os.path.join(work_dir, 'data', target_dir, prefix_str)\n",
    "    os.makedirs(dest_dir)\n",
    "    for image_path in image_paths:\n",
    "        shutil.copy(image_path, dest_dir)\n",
    "\n",
    "try:\n",
    "    copy_files('dog', 1, 1001, 'train')\n",
    "    copy_files('cat', 1, 1001, 'train')\n",
    "    copy_files('dog', 1001, 1401, 'test')\n",
    "    copy_files('cat', 1001, 1401, 'test')\n",
    "    print(\"Data is prepared\")\n",
    "    \n",
    "except FileExistsError:\n",
    "    print(\"Data is prepared\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "62/62 [==============================] - 8s 125ms/step - loss: 0.7284 - acc: 0.4990 - val_loss: 0.6866 - val_acc: 0.5713\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 5s 84ms/step - loss: 0.6927 - acc: 0.5318 - val_loss: 0.6824 - val_acc: 0.5437\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 5s 85ms/step - loss: 0.6863 - acc: 0.5580 - val_loss: 0.6641 - val_acc: 0.5900\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 5s 84ms/step - loss: 0.6552 - acc: 0.6275 - val_loss: 0.6634 - val_acc: 0.6025\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 5s 84ms/step - loss: 0.6449 - acc: 0.6351 - val_loss: 0.6372 - val_acc: 0.6225\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 5s 82ms/step - loss: 0.6424 - acc: 0.6422 - val_loss: 0.6405 - val_acc: 0.5950\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 5s 82ms/step - loss: 0.6258 - acc: 0.6663 - val_loss: 0.6280 - val_acc: 0.6425\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 5s 82ms/step - loss: 0.6138 - acc: 0.6593 - val_loss: 0.6124 - val_acc: 0.6475\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 5s 80ms/step - loss: 0.5970 - acc: 0.6825 - val_loss: 0.6109 - val_acc: 0.6613\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 5s 81ms/step - loss: 0.5703 - acc: 0.6971 - val_loss: 0.5908 - val_acc: 0.6913\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 5s 82ms/step - loss: 0.5933 - acc: 0.6799 - val_loss: 0.5980 - val_acc: 0.6700\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 5s 82ms/step - loss: 0.5715 - acc: 0.6981 - val_loss: 0.5844 - val_acc: 0.6575\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 5s 83ms/step - loss: 0.5356 - acc: 0.7394 - val_loss: 0.5821 - val_acc: 0.6713\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 5s 84ms/step - loss: 0.5372 - acc: 0.7369 - val_loss: 0.5908 - val_acc: 0.6700\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 5s 80ms/step - loss: 0.5277 - acc: 0.7374 - val_loss: 0.5683 - val_acc: 0.7050\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 5s 79ms/step - loss: 0.5251 - acc: 0.7470 - val_loss: 0.5789 - val_acc: 0.7100\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 5s 77ms/step - loss: 0.5002 - acc: 0.7565 - val_loss: 0.5344 - val_acc: 0.7362\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 5s 81ms/step - loss: 0.4935 - acc: 0.7591 - val_loss: 0.5420 - val_acc: 0.7362\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 5s 77ms/step - loss: 0.4711 - acc: 0.7752 - val_loss: 0.5521 - val_acc: 0.7225\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 5s 76ms/step - loss: 0.5215 - acc: 0.7404 - val_loss: 0.6187 - val_acc: 0.6737\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 5s 76ms/step - loss: 0.4701 - acc: 0.7757 - val_loss: 0.5539 - val_acc: 0.7262\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.4515 - acc: 0.7913 - val_loss: 0.5544 - val_acc: 0.7388\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.4614 - acc: 0.7848 - val_loss: 0.5407 - val_acc: 0.7275\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.4300 - acc: 0.7974 - val_loss: 0.6023 - val_acc: 0.7163\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.4479 - acc: 0.7873 - val_loss: 0.5608 - val_acc: 0.7400\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 5s 73ms/step - loss: 0.4123 - acc: 0.8029 - val_loss: 0.6062 - val_acc: 0.7075\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.3996 - acc: 0.8206 - val_loss: 0.5753 - val_acc: 0.7412\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.3694 - acc: 0.8417 - val_loss: 0.6543 - val_acc: 0.6763\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 5s 73ms/step - loss: 0.3488 - acc: 0.8488 - val_loss: 0.6055 - val_acc: 0.7338\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.3859 - acc: 0.8306 - val_loss: 0.7060 - val_acc: 0.6925\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.3325 - acc: 0.8503 - val_loss: 0.6124 - val_acc: 0.7338\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 4s 73ms/step - loss: 0.3247 - acc: 0.8664 - val_loss: 0.6115 - val_acc: 0.7438\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.3070 - acc: 0.8679 - val_loss: 0.6806 - val_acc: 0.7200\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.2972 - acc: 0.8725 - val_loss: 0.6175 - val_acc: 0.7163\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.3036 - acc: 0.8694 - val_loss: 0.6339 - val_acc: 0.7163\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.2689 - acc: 0.8942 - val_loss: 0.6610 - val_acc: 0.7037\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.2788 - acc: 0.8790 - val_loss: 0.6289 - val_acc: 0.7262\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.2661 - acc: 0.8881 - val_loss: 0.6577 - val_acc: 0.7375\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.2580 - acc: 0.9032 - val_loss: 0.6904 - val_acc: 0.7188\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.2280 - acc: 0.9052 - val_loss: 0.7053 - val_acc: 0.7362\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.2101 - acc: 0.9138 - val_loss: 0.6964 - val_acc: 0.7175\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.2291 - acc: 0.9143 - val_loss: 0.6314 - val_acc: 0.7488\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.1941 - acc: 0.9244 - val_loss: 0.6927 - val_acc: 0.7475\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.2193 - acc: 0.9188 - val_loss: 0.6648 - val_acc: 0.7375\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 5s 74ms/step - loss: 0.2121 - acc: 0.9133 - val_loss: 0.6892 - val_acc: 0.7212\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 5s 78ms/step - loss: 0.1947 - acc: 0.9229 - val_loss: 0.6712 - val_acc: 0.7525\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.1833 - acc: 0.9219 - val_loss: 0.8932 - val_acc: 0.6887\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 5s 77ms/step - loss: 0.1990 - acc: 0.9219 - val_loss: 0.6962 - val_acc: 0.7475\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 5s 78ms/step - loss: 0.1849 - acc: 0.9289 - val_loss: 0.7773 - val_acc: 0.7350\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 5s 80ms/step - loss: 0.1428 - acc: 0.9466 - val_loss: 0.7829 - val_acc: 0.7425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f515ef4b198>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "work_dir = 'dataset/data'\n",
    "\n",
    "\n",
    "image_height, image_width = 32, 32 # image resize\n",
    "train_dir = os.path.join(work_dir, 'train')\n",
    "test_dir = os.path.join(work_dir, 'test')\n",
    "no_classes = 2\n",
    "no_validation = 800\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "no_train = 2000\n",
    "no_test = 800\n",
    "input_shape = (image_height, image_width, 3)\n",
    "epoch_steps = no_train // batch_size\n",
    "test_steps = no_test // batch_size\n",
    "\n",
    "def simple_cnn(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Dense(units=no_classes, activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "simple_cnn_model = simple_cnn(input_shape)\n",
    "\n",
    "generator_train = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.3,\n",
    "    shear_range=0.3,)\n",
    "\n",
    "generator_test = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_images = generator_train.flow_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_width, image_height))\n",
    "\n",
    "test_images = generator_test.flow_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_width, image_height))\n",
    "\n",
    "simple_cnn_model.fit_generator(\n",
    "    train_images,\n",
    "    steps_per_epoch=epoch_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_images,\n",
    "    validation_steps=test_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
