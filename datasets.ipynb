{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pedestrian:  \n",
    "http://pascal.inrialpes.fr/data/human/\n",
    "\n",
    "Pedestrian Detection:  \n",
    "https://github.com/diegocavalca/machine-learning/blob/master/supervisioned/object.detection_tensorflow/simple.detection.ipynb\n",
    "\n",
    "Traffic Signs:  \n",
    "http://www.vision.ee.ethz.ch/~timofter/traffic_signs/  \n",
    "\n",
    "http://btsd.ethz.ch/shareddata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biomedical Image Segmentation:  \n",
    "http://www.cs.bu.edu/~betke/BiomedicalImageSegmentation\n",
    "\n",
    "challenge datasets:  \n",
    "https://grand-challenge.org/all_challenges\n",
    "\n",
    "Cornell University public image:  \n",
    "http://www.via.cornell.edu/databases/\n",
    "\n",
    "SpaceNet on AWS:  \n",
    "https://registry.opendata.aws/spacenet/\n",
    "\n",
    "Can you build algorithms to classify facility, building, and land use from satellite imagery:  \n",
    "https://www.iarpa.gov/challenges/fmow.html\n",
    "\n",
    "Use satellite data to track the human footprint in the Amazon rainforest:  \n",
    "https://www.kaggle.com/c/planet-understanding-the-amazon-from-space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mobile annotation tool:  \n",
    "http://labelme.csail.mit.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human face detection:  \n",
    "http://vis-www.cs.umass.edu/fddb/\n",
    "\n",
    "http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/\n",
    "\n",
    "http://www.cbsr.ia.ac.cn/faceevaluation/\n",
    "\n",
    "http://mmlab.ie.cuhk.edu.hk/projects/TCDCN/data/MTFL.zip\n",
    "\n",
    "https://www.kaggle.com/c/facial-keypoints-detection/data\n",
    "\n",
    "https://github.com/zhzhanp/TCDCN-face-alignment\n",
    "\n",
    "Face recognition:  \n",
    "http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "https://www.cs.tau.ac.il/~wolf/ytfaces/\n",
    "\n",
    "http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "\n",
    "http://www.robots.ox.ac.uk/~vgg/data/vgg_face2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Recognition:  \n",
    "http://crcv.ucf.edu/data/UCF101.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YouTube-8M is a large-scale labeled video dataset that consists of millions of YouTube video IDs, with high-quality machine-generated annotations from a diverse vocabulary of 3,800+ visual entities:  \n",
    "https://research.google.com/youtube8m/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Classification Problems:  \n",
    "http://cs.stanford.edu/people/karpathy/deepvideo/\n",
    "\n",
    "http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/\n",
    "\n",
    "http://www.di.ens.fr/~laptev/actions/hollywood2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human Pose Estimation:  \n",
    "https://lear.inrialpes.fr/research/posesinthewild/\n",
    "\n",
    "https://bensapp.github.io/flic-dataset.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Captioning:  \n",
    "\n",
    "https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/mpii-movie-description-dataset/\n",
    "\n",
    "https://mila.quebec/en/publications/public-datasets/m-vad/\n",
    "\n",
    "(YouTube2Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
